WEBVTT

1
00:00:02.279 --> 00:00:06.144
So what if you want to read a little
bit more about the discussion of

2
00:00:06.144 --> 00:00:10.100
the differences between machinery and
traditional statistics.

3
00:00:10.100 --> 00:00:13.900
Well, I am going to give you a couple of
articles that you can go get and read.

4
00:00:13.900 --> 00:00:15.800
One is a blog post by Larry Wasserman.

5
00:00:15.800 --> 00:00:17.740
He is a well known statistician, and

6
00:00:17.740 --> 00:00:21.710
he wrote a blog post if you search
Called the Rise of the Machines.

7
00:00:21.710 --> 00:00:25.500
It's a very good treatment
of the distinction and

8
00:00:25.500 --> 00:00:29.570
maybe a little bit of a rallying
cry to the statistical community to

9
00:00:29.570 --> 00:00:32.310
embrace machine learning
a little bit better.

10
00:00:32.310 --> 00:00:36.650
There was this very celebrated
statistician who sadly passed away,

11
00:00:36.650 --> 00:00:38.780
named Leo Breiman.

12
00:00:38.780 --> 00:00:41.300
And he wrote a tremendously
statistical paper

13
00:00:41.300 --> 00:00:44.000
called Statistical Modeling:
The Two Cultures, and

14
00:00:44.000 --> 00:00:47.370
much of my thinking really
derives from this paper.

15
00:00:47.370 --> 00:00:48.400
It's very accessible.

16
00:00:48.400 --> 00:00:51.720
It's in the Journal of
Statistical Science, and

17
00:00:51.720 --> 00:00:55.740
I would say his approach is quite
critical of traditional statistics, or

18
00:00:55.740 --> 00:01:00.450
at least suggesting that traditional
statistics is lagging behind.

19
00:01:00.450 --> 00:01:05.300
I give you this quote here, where he says,
in this paper I will argue that the focus

20
00:01:05.300 --> 00:01:09.970
in the statistical community on data
models has led to irrelevant theory.

21
00:01:09.970 --> 00:01:13.880
Kept statisticians from using more
suitable algorithmic models and

22
00:01:13.880 --> 00:01:16.460
prevented statisticians from
working on exciting new problems.

23
00:01:16.460 --> 00:01:19.700
So, he kind of takes his own
community to task quite a bit.

24
00:01:19.700 --> 00:01:21.330
It's a very entertaining article.

25
00:01:21.330 --> 00:01:25.490
And it's not quite as dry and
academic as most scientific,

26
00:01:25.490 --> 00:01:30.590
most statistical research papers, and

27
00:01:30.590 --> 00:01:35.090
I just brought out this wonderful quote
that really stuck with me from D.R. Cox.

28
00:01:35.090 --> 00:01:39.210
D.R. Cox is another tremendously
celebrated statistician,

29
00:01:39.210 --> 00:01:43.240
his Cox proportional hazard
model is easily one of the top

30
00:01:43.240 --> 00:01:45.488
five most cited statistical papers.

31
00:01:45.488 --> 00:01:52.480
And Dr. Cox was one of the discussants
of the paper, and this quote really

32
00:01:52.480 --> 00:01:56.780
stuck with me, where he said, Professor
Breiman takes data as his starting point.

33
00:01:56.780 --> 00:02:00.480
I would prefer to start with an issue,
a question, or a scientific hypothesis.

34
00:02:00.480 --> 00:02:03.690
Although I would be surprised if this
were a real source of disagreement.

35
00:02:03.690 --> 00:02:09.480
But I really like this comment because
I do think that really does get at to

36
00:02:09.480 --> 00:02:14.070
me, I think of the things I have the most
trouble with with machine learning.

37
00:02:14.070 --> 00:02:16.780
It's when you're having to both derive and

38
00:02:16.780 --> 00:02:19.920
interrogate the hypothesis
with the same set of data.

39
00:02:19.920 --> 00:02:23.059
I think to me,
that's the hardest pill to swallow.

40
00:02:24.400 --> 00:02:27.640
And then the final article that you
might be interested in looking at

41
00:02:27.640 --> 00:02:32.220
was an article by David Hand,
another great statistician.

42
00:02:32.220 --> 00:02:37.000
And he says Classifier Technology and
the Illusion of Progress, so

43
00:02:37.000 --> 00:02:40.400
this one's a little bit more critical
of the machine learning approach, and

44
00:02:40.400 --> 00:02:41.830
I'll give you a quote from this.

45
00:02:41.830 --> 00:02:44.560
It says, so that, so that the apparent

46
00:02:44.560 --> 00:02:48.320
superiority of more sophisticated
methods may be something of an illusion.

47
00:02:48.320 --> 00:02:52.034
In particular simpler methods typically
yield performance almost as good as more

48
00:02:52.034 --> 00:02:55.693
sophisticated methods, to the extent that
the difference in performance may be

49
00:02:55.693 --> 00:02:59.191
swamped by other sources of uncertainty
that generally are not considered in

50
00:02:59.191 --> 00:03:01.705
the classical supervised
classification paradigm.

51
00:03:01.705 --> 00:03:06.715
So he's basically saying the effort
of building a complicated

52
00:03:06.715 --> 00:03:10.940
machine learning algorithm
is often not worth it.

53
00:03:10.940 --> 00:03:15.340
The increase in complexity
leads to marginal gains is

54
00:03:15.340 --> 00:03:17.440
one of the points that he is making.

55
00:03:17.440 --> 00:03:23.480
So it's another interesting paper, quite
the opposite of say Leo Breiman's paper.

56
00:03:23.480 --> 00:03:27.730
So that gives you, hopefully, enough
to go on for the rest of the course.

57
00:03:27.730 --> 00:03:30.470
We're really gonna kinda focus
on more traditional analysis,

58
00:03:30.470 --> 00:03:32.750
under the assumption that
you're going to be using and

59
00:03:32.750 --> 00:03:38.325
managing data scientists to try and
come up with parsimonious explanations for

60
00:03:38.325 --> 00:03:41.665
your industry or
whatever it is you're trying to study.

61
00:03:41.665 --> 00:03:44.336
So thanks for listening and I look forward
to seeing you in the next lecture.