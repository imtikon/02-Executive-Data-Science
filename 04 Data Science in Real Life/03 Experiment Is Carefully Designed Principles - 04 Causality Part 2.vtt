WEBVTT

1
00:00:03.100 --> 00:00:06.080
Another way that people get at,
this is natural experiments.

2
00:00:07.150 --> 00:00:12.870
So a great example of natural
experiments is where smoking trials.

3
00:00:12.870 --> 00:00:16.220
So you can't randomize people some
to smoke, and some not to smoke, so

4
00:00:16.220 --> 00:00:22.510
you can't get the kind of Cadillac version
of a causal fact via randomization.

5
00:00:22.510 --> 00:00:27.760
What you can look at is, for example,
places that put in smoking bans.

6
00:00:27.760 --> 00:00:30.980
And smoking bans impact smoking behavior.

7
00:00:30.980 --> 00:00:34.820
Right?
So, you can maybe accurate numbers of

8
00:00:34.820 --> 00:00:41.630
smoking related cardiac
issues in a city or

9
00:00:41.630 --> 00:00:47.710
some other geographic unit, and before and
after a smoking ban went into effect.

10
00:00:47.710 --> 00:00:48.520
Okay?

11
00:00:48.520 --> 00:00:51.620
And that gets at the idea of causality for

12
00:00:51.620 --> 00:00:53.520
the same reasons we were
talking about before.

13
00:00:53.520 --> 00:00:56.420
But of course, it fails with
some fundamental assumptions.

14
00:00:56.420 --> 00:00:58.050
You have to aggregate it a high level.

15
00:00:58.050 --> 00:00:59.290
You don't get individuals.

16
00:00:59.290 --> 00:01:02.040
You certainly don't get
individual counter factuals.

17
00:01:02.040 --> 00:01:07.140
And everything that's aliased with the
timing of before the law to after the law

18
00:01:07.140 --> 00:01:09.590
could contaminate our results.

19
00:01:09.590 --> 00:01:13.930
So if in the same election cycle where

20
00:01:13.930 --> 00:01:18.299
the smoking ban was passed that broadly
impacted people's smoking behaviors.

21
00:01:19.350 --> 00:01:24.010
If that same election cycle had other
policies that also impacted things,

22
00:01:24.010 --> 00:01:30.140
those other policies would also be
associated with whatever cardiac decline,

23
00:01:30.140 --> 00:01:33.570
decline in cardiac issues that you
saw from your hospital records.

24
00:01:34.800 --> 00:01:39.350
So again, naturalized natural experiments,
something where some external manipulation

25
00:01:39.350 --> 00:01:43.880
like a ban going into effect,
those are another way you can try to get

26
00:01:43.880 --> 00:01:48.590
that causality, again, but
with a lot of assumptions.

27
00:01:48.590 --> 00:01:51.750
And again, the entire field of causal
infringement is trying to outline those

28
00:01:51.750 --> 00:01:54.540
assumptions and again, in this case, the
assumptions, I think, are quite strong.

29
00:01:56.150 --> 00:01:58.430
Matching is another great one and, so

30
00:01:58.430 --> 00:02:01.600
matching is this idea of
finding Dopplegangers.

31
00:02:01.600 --> 00:02:06.110
So here I have a picture of
President Obama, and then I looked up

32
00:02:06.110 --> 00:02:10.860
Doppelganger for President Obama on
Google, and it found this fellow.

33
00:02:12.030 --> 00:02:14.430
So what's the idea of matching?

34
00:02:14.430 --> 00:02:19.335
So if I have a collection of subjects
that received the treatment,

35
00:02:19.335 --> 00:02:24.815
I find people who

36
00:02:24.815 --> 00:02:32.610
are very close in every other respect
control subjects and match them up.

37
00:02:32.610 --> 00:02:37.090
So as an example, this is very
commonly done in medical studies.

38
00:02:37.090 --> 00:02:42.450
So if you found, for
example you could look retrospectively,

39
00:02:42.450 --> 00:02:46.600
if you found a bunch of cases of
people that died of lung cancer.

40
00:02:47.640 --> 00:02:51.630
And you had a complete medical history and
a lot of other history on these subjects,

41
00:02:51.630 --> 00:02:55.940
then you would find some other people
to match and contrast them with that

42
00:02:58.180 --> 00:03:03.180
never had lung cancer, but
that had the same weight, same age, etc.

43
00:03:03.180 --> 00:03:04.170
etc.
etc.

44
00:03:04.170 --> 00:03:10.250
So at any rate,
think when you think about matching,

45
00:03:10.250 --> 00:03:13.580
think about why when people do this,
why they're thinking causally.

46
00:03:13.580 --> 00:03:17.100
They're trying to get that counter
factual idea by saying okay,

47
00:03:17.100 --> 00:03:20.882
I can't observe this person twice
in two states of nature, but

48
00:03:20.882 --> 00:03:24.430
me, I can observe this person and
they're close Doppelganger.

49
00:03:24.430 --> 00:03:28.940
Of course, the real problem with matching,
well there's a couple,

50
00:03:28.940 --> 00:03:31.590
but the main problem with matching is

51
00:03:31.590 --> 00:03:34.950
you're only as good as those
things that you can match on.

52
00:03:34.950 --> 00:03:36.690
Right?
And then there's also the problem of

53
00:03:36.690 --> 00:03:37.480
finding matches.

54
00:03:37.480 --> 00:03:41.520
You may not be able to find the data set
to match enough characteristics, but

55
00:03:41.520 --> 00:03:43.120
you're only as good as
those things you match on.

56
00:03:43.120 --> 00:03:46.990
Everything you fail to match on,
than you haven't found a real,

57
00:03:46.990 --> 00:03:50.109
you haven't found the same person
in the important respects.

58
00:03:51.310 --> 00:03:52.360
There's other problems.

59
00:03:52.360 --> 00:03:55.540
Often, if you have a data set, and
you can find matches for some people and

60
00:03:55.540 --> 00:03:59.060
you throw out the remainder because
you can't find good matches for them,

61
00:03:59.060 --> 00:04:02.030
well then you've kind of destroyed some
of the generalizability of your sample.

62
00:04:02.030 --> 00:04:03.730
If you have a really good sample, and

63
00:04:03.730 --> 00:04:07.760
then now you're only analyzing
the people that have matches,

64
00:04:07.760 --> 00:04:12.090
that new sample may not be generalizable
in the way the original sample was.

65
00:04:12.090 --> 00:04:15.910
So again, in all cases when you're
trying to make causal assumptions,

66
00:04:15.910 --> 00:04:17.980
you're going to have to bite some bullets.

67
00:04:17.980 --> 00:04:22.000
But I would hope that you can see
the connection of how people are trying to

68
00:04:22.000 --> 00:04:25.990
think causally and think about
counterfactuals when they do matching.

69
00:04:27.510 --> 00:04:30.800
Randomization is our best tool.

70
00:04:30.800 --> 00:04:34.120
And it's our most effective tool for
estimating average causal effect.

71
00:04:34.120 --> 00:04:38.380
So we have an entire class and
lecture on randomization in AB testing.

72
00:04:38.380 --> 00:04:41.790
But the idea of randomization
is to make the treated and

73
00:04:41.790 --> 00:04:44.910
untreated groups as directly
comparable as possible.

74
00:04:44.910 --> 00:04:49.270
So if you have a group of people and
you randomize the twist and

75
00:04:49.270 --> 00:04:53.880
tone to half of them, and the other half
of them don't get anything, well yes,

76
00:04:53.880 --> 00:04:59.860
you don't have exact matches of people who
receive the controls to people who didn't.

77
00:04:59.860 --> 00:05:03.050
But in all other respects by
virtue of the randomization,

78
00:05:03.050 --> 00:05:07.380
they should with high probability,
the groups should be directly comparable.

79
00:05:07.380 --> 00:05:12.580
And you can show via some mathematics
that if the sample is large enough and

80
00:05:12.580 --> 00:05:17.170
it's a random sample, you do really
get two average causal effects.

81
00:05:17.170 --> 00:05:21.040
This doesn't mean that randomization
is a panacea, there can be issues.

82
00:05:21.040 --> 00:05:24.060
For example, there might be dropout,

83
00:05:24.060 --> 00:05:26.330
people get randomized to
the twisting zone, but

84
00:05:26.330 --> 00:05:29.750
they realize they look kinda ridiculous
doing it, so they stop doing it.

85
00:05:29.750 --> 00:05:33.440
So the collection of people that
complete the treatment are different

86
00:05:33.440 --> 00:05:36.490
than the people that were randomized
to the treatment, so there's

87
00:05:36.490 --> 00:05:41.810
lots of issues that can come up, even in
our Cadillac version of randomized trials.

88
00:05:41.810 --> 00:05:46.641
But this is to date the best way that
we can get that causal and effect.

89
00:05:47.950 --> 00:05:49.990
So what are some other
examples of things we can do?

90
00:05:51.510 --> 00:05:56.120
Instrumental variables are a very clever
idea, and we're not gonna go through

91
00:05:56.120 --> 00:05:59.640
the details, but imagine if you
wanted to understand whether or

92
00:05:59.640 --> 00:06:04.480
not oral contraceptives had
a relationship with ovarian cancer.

93
00:06:04.480 --> 00:06:05.640
Very reasonable question.

94
00:06:07.330 --> 00:06:11.870
So, and let's suppose you

95
00:06:11.870 --> 00:06:16.860
you looked at something like reimbursement
rates as those relate to ovarian cancer.

96
00:06:18.400 --> 00:06:23.230
Well the fact that there's no direct
link between reimbursement rates and

97
00:06:23.230 --> 00:06:24.460
ovarian cancer.

98
00:06:24.460 --> 00:06:26.920
You know, how we elect to

99
00:06:26.920 --> 00:06:30.850
reimburse oral contraceptive
usage through insurance plans.

100
00:06:30.850 --> 00:06:37.560
Can't possibly have any real relationship
with ovarian cancer, so any effect of

101
00:06:37.560 --> 00:06:43.780
reimbursement rates on ovarian cancer
has to go through contraceptive use.

102
00:06:43.780 --> 00:06:45.590
So people have used this idea.

103
00:06:45.590 --> 00:06:50.540
The reimbursement rate is an instrumental
variable so people have used this idea to

104
00:06:50.540 --> 00:06:56.280
try and come causal effects without
actually having randomization.

105
00:06:56.280 --> 00:07:00.480
There's a big assumption in instrumental
variables that the only way in which

106
00:07:00.480 --> 00:07:04.990
reimbursement rates could get to ovarian
cancer is through oral contraceptive use.

107
00:07:04.990 --> 00:07:11.870
Again, that assumption is often that
the only link between the instrumental

108
00:07:11.870 --> 00:07:15.590
variable and the outcome is through
this variable that you're interested in.

109
00:07:16.910 --> 00:07:20.380
That assumption is often suspect as it's
very hard to find instrumental variables.

110
00:07:20.380 --> 00:07:24.760
But this was an incredibly clever idea
it underlies a lot of causal thinking,

111
00:07:24.760 --> 00:07:27.920
modern causal thinking, and
it's a pretty interesting approach.

112
00:07:28.960 --> 00:07:30.110
And then there's always modeling.

113
00:07:31.260 --> 00:07:33.820
So in modeling you're trying to,
for example,

114
00:07:33.820 --> 00:07:38.440
build up a model that explains why
people respond to the treatment or not.

115
00:07:38.440 --> 00:07:40.730
And if that is model
sort of all encompassing,

116
00:07:40.730 --> 00:07:43.670
then you don't need to see
the person in two states of nature.

117
00:07:43.670 --> 00:07:48.350
You have in a sense the two states of
nature because you've built up the system,

118
00:07:48.350 --> 00:07:51.260
you've understood the system
through the model.

119
00:07:51.260 --> 00:07:55.470
Again, this is only as good as the model
is, it's fraught with assumptions.

120
00:07:55.470 --> 00:08:00.430
And so, and I would just say
the most well known modeling

121
00:08:00.430 --> 00:08:04.400
technique in causal inferences
uses so-called propensity scores.

122
00:08:04.400 --> 00:08:09.945
So what I've given you in this lecture
is one definition of causality,

123
00:08:09.945 --> 00:08:13.970
this counter factual
definition of causality.

124
00:08:13.970 --> 00:08:17.310
One generalization of the idea,
the average causal effect.

125
00:08:17.310 --> 00:08:21.590
And how different ways of statistical
thinking try to get at this.

126
00:08:21.590 --> 00:08:26.510
And we try to understand both that
these ways of statistical thinking

127
00:08:26.510 --> 00:08:30.680
get at causal effects, but they come
with a lot of assumption and baggage.

128
00:08:30.680 --> 00:08:35.350
And that there's often no perfect
way to get at causal effects.

129
00:08:35.350 --> 00:08:38.070
And the best thing we have
going is randomization.

130
00:08:39.280 --> 00:08:41.510
So if you'd like to read
more about causality,

131
00:08:41.510 --> 00:08:45.560
actually the Wikipedia page on
causal inferences is pretty good.

132
00:08:45.560 --> 00:08:50.740
And I think any of the survey papers by,

133
00:08:50.740 --> 00:08:54.850
for example, Donald Rubin,
are wonderful reads on causal inferences.

134
00:08:54.850 --> 00:08:56.569
He's one of the modern forefathers.

135
00:08:57.760 --> 00:09:01.530
Of the renewed interest in
causal thinking and statistics.

136
00:09:02.870 --> 00:09:05.144
Well thanks for listening, I look forward
to seeing you in the next lecture.