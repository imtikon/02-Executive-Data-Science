WEBVTT

1
00:00:01.840 --> 00:00:05.490
Hi, my name is Brian Caffo, and
this is the lecture on multiplicity and

2
00:00:05.490 --> 00:00:07.570
multiple comparisons.

3
00:00:07.570 --> 00:00:09.350
So in this lecture we're gonna go fishing.

4
00:00:11.480 --> 00:00:17.200
And as a homework problem, I'd like you to
read this XKCD comic, comic number 882.

5
00:00:17.200 --> 00:00:22.050
And it's really a great description
of multiple comparisons and

6
00:00:22.050 --> 00:00:26.975
so I won't rehash it here cuz it's
a long one, but go read that.

7
00:00:26.975 --> 00:00:27.860
That's part of your homework.

8
00:00:27.860 --> 00:00:29.540
It's pretty funny.

9
00:00:29.540 --> 00:00:31.190
So what is multiple comparisons?

10
00:00:31.190 --> 00:00:35.510
Multiple comparisons is
the idea of repeatedly doing

11
00:00:35.510 --> 00:00:40.530
hypothesis tests until
one comes up significant.

12
00:00:40.530 --> 00:00:41.070
Okay?

13
00:00:41.070 --> 00:00:43.740
And this sounds nefarious, but

14
00:00:43.740 --> 00:00:47.710
it can be often done with
the best of intentions, right?

15
00:00:47.710 --> 00:00:52.580
And multiple comparisons issues
can creep in in unexpected ways.

16
00:00:53.890 --> 00:00:56.970
So to just give you an example
of multiple comparisons,

17
00:00:56.970 --> 00:01:02.050
that's sort of famous in the field that I
work in, is someone took a dead salmon and

18
00:01:02.050 --> 00:01:04.630
put it in a functional magnetic
resonance imaging scanner.

19
00:01:05.720 --> 00:01:10.160
And they ran the pixel by pixel analysis,

20
00:01:10.160 --> 00:01:14.660
actually it's three dimensions so it's
often called a voxel instead of a pixel.

21
00:01:14.660 --> 00:01:19.670
They ran a voxel by voxel analysis and
checked for

22
00:01:19.670 --> 00:01:22.450
a statistical significance
at each voxel of whether or

23
00:01:22.450 --> 00:01:25.930
not brain activation existed, and
they found significant things.

24
00:01:25.930 --> 00:01:30.522
And the reason they found significant
things is because of a lack of

25
00:01:30.522 --> 00:01:33.970
adjustment for multiple comparisons.

26
00:01:33.970 --> 00:01:36.739
And of course, there was no brain
activation because it was a dead salmon.

27
00:01:38.980 --> 00:01:43.983
So multiple comparisons, this basically
just arises from applying many tests and

28
00:01:43.983 --> 00:01:46.130
focusing on the significant ones.

29
00:01:46.130 --> 00:01:50.513
That's the negative aspect
of multiple comparisons, but

30
00:01:50.513 --> 00:01:56.558
basically the idea of multiple comparisons
exists just from doing a lot of tests,

31
00:01:56.558 --> 00:01:58.835
even if all the tests are null,

32
00:01:58.835 --> 00:02:04.738
the fact that you will just get some
rejections by chance, just by randomness.

33
00:02:04.738 --> 00:02:07.398
So this can arise from
fitting too many models,

34
00:02:07.398 --> 00:02:10.710
this can arise from looking at
too many quantities of interest,

35
00:02:10.710 --> 00:02:15.910
like in our fish example they just
simply looked at too many voxels, or

36
00:02:15.910 --> 00:02:19.320
not too many they just didn't
appropriately control for it.

37
00:02:19.320 --> 00:02:22.300
Or I think the nefarious version
of multiple comparisons,

38
00:02:22.300 --> 00:02:25.590
the most nefarious version,
is fishing expeditions.

39
00:02:25.590 --> 00:02:31.220
And that's basically testing hypotheses,
testing everything without a priori

40
00:02:31.220 --> 00:02:37.190
hypotheses and just trying to focus in
on those ones that are just significant.

41
00:02:37.190 --> 00:02:40.500
So that's the most negative
version of multiple comparisons.

42
00:02:40.500 --> 00:02:44.200
You have a large dataset,
you want to explain an outcome.

43
00:02:44.200 --> 00:02:49.190
You look at every variable you have,
every combination of variables

44
00:02:49.190 --> 00:02:53.130
with lots of models, pick out the small
subset of significant ones, and

45
00:02:53.130 --> 00:02:56.130
report them as if you hadn't
looked at all the others.

46
00:02:56.130 --> 00:02:58.780
And I think everyone can agree
that there's some problem

47
00:02:58.780 --> 00:03:00.570
with doing something like that.

48
00:03:00.570 --> 00:03:05.160
But I also want to emphasize,
multiple comparisons is not intrinsically,

49
00:03:05.160 --> 00:03:09.640
in looking at lots of hypothesis tests,
is not intrinsically a bad thing to do.

50
00:03:09.640 --> 00:03:15.250
The bad thing to do is to do a lot of
hypothesis tests and not be up front and

51
00:03:15.250 --> 00:03:18.090
account for the fact that you've
done a lot of hypothesis tests, and

52
00:03:18.090 --> 00:03:20.200
that's basically the point
of this lecture.

53
00:03:21.840 --> 00:03:24.070
And I want to give you
the easiest fix possible.

54
00:03:24.070 --> 00:03:29.590
There is a vast literature on multiple
comparisons, but the easiest fix uses

55
00:03:29.590 --> 00:03:34.490
an inequality that's named after
the mathematician, Bonferroni.

56
00:03:35.670 --> 00:03:40.290
And because it's named
after his inequality,

57
00:03:40.290 --> 00:03:42.950
it's called the Bonferroni correction.

58
00:03:42.950 --> 00:03:47.400
And the easiest way to execute
a Bonferroni correction is if you

59
00:03:47.400 --> 00:03:49.270
have a bunch of P-values,

60
00:03:49.270 --> 00:03:53.790
simply multiply your P-values by the
number of tests that you're performing.

61
00:03:53.790 --> 00:03:57.180
So, for example,
if you perform ten tests and

62
00:03:57.180 --> 00:04:03.700
one of the P-values is 0.01,
then that P-value is now 0.10.

63
00:04:03.700 --> 00:04:05.730
Okay, cuz you've multiplied it by ten.

64
00:04:05.730 --> 00:04:12.490
So the Bonferroni correction is highly
robust and it works in all circumstances.

65
00:04:12.490 --> 00:04:15.220
The only problem is,
is it's quite conservative.

66
00:04:15.220 --> 00:04:19.312
So you can imagine, if you perform
a thousand tests, you're multiplying all

67
00:04:19.312 --> 00:04:23.970
your p values by a thousand, it's unlikely
you're going to get anything significant.

68
00:04:23.970 --> 00:04:30.620
So the result of the Bonferroni correction
is often that it is very conservative,

69
00:04:30.620 --> 00:04:35.020
tends to err on the side of
declaring fewer things significant.

70
00:04:36.390 --> 00:04:39.820
But it's the easiest thing and then if
you want to get into harder versions of

71
00:04:39.820 --> 00:04:42.492
multiple comparisons,
you can look into further reading.

72
00:04:42.492 --> 00:04:48.740
You can read up on things like
false discovery rates and

73
00:04:48.740 --> 00:04:54.090
things like, we actually had a relatively
well-known multiple comparisons paper come

74
00:04:54.090 --> 00:05:00.130
out of this department from the faculty
member named David Duncan, and Duncan's

75
00:05:00.130 --> 00:05:05.020
multiple range test is an example in ANOVA
of multiple comparisons corrections.

76
00:05:05.020 --> 00:05:08.590
But I think all of those are much harder
than the simple Bonferroni correction so

77
00:05:08.590 --> 00:05:10.700
that's what I would go with now.

78
00:05:10.700 --> 00:05:15.950
One thing I would mention about
the Bonferroni correction is that

79
00:05:15.950 --> 00:05:20.560
it doesn't give you any guidance
on what number to multiply by.

80
00:05:20.560 --> 00:05:22.110
So if you do a bunch of tests,

81
00:05:22.110 --> 00:05:27.310
and some are related and another set
are kind of unrelated, they're two groups.

82
00:05:27.310 --> 00:05:32.770
Should you aggregate all of those tests
together, and count, say, if you did ten

83
00:05:32.770 --> 00:05:36.920
tests of this sort, and ten tests of this
sort for a totally different hypothesis.

84
00:05:36.920 --> 00:05:40.770
Do you aggregate all of them together,
and multiply all your p values by 20?

85
00:05:40.770 --> 00:05:44.730
Or you just say, well this is one
separate investigation from this

86
00:05:44.730 --> 00:05:49.770
other separate investigation and so
we multiply these by ten and these by ten.

87
00:05:49.770 --> 00:05:52.950
The Bonferroni correction gives
no guidance on this problem.

88
00:05:52.950 --> 00:05:56.730
The Bonferroni correction doesn't tell
you what is the right number of tests to

89
00:05:56.730 --> 00:05:59.350
multiply by, and
that is an inherent problem.

90
00:05:59.350 --> 00:06:04.600
So the best advice I can give on
that is to be your own critic.

91
00:06:04.600 --> 00:06:07.160
Be a critic of the results
that you have and

92
00:06:07.160 --> 00:06:12.460
include all the tests that you think a
critic of yours would want you to include

93
00:06:12.460 --> 00:06:14.770
as penalizing for
having looked at the data.

94
00:06:16.390 --> 00:06:20.140
Okay, so again, I would just reiterate
there's a vast literature on some other

95
00:06:20.140 --> 00:06:23.600
techniques for
handling multiple comparisons, but again,

96
00:06:23.600 --> 00:06:27.030
the Bonferroni correction is
the simplest and easiest one.

97
00:06:27.030 --> 00:06:30.290
And if you're managing someone
who's done a lot of tests or

98
00:06:30.290 --> 00:06:31.655
looked at a lot of models,

99
00:06:31.655 --> 00:06:35.941
fit a lot of models, the easiest thing to
do is when you're looking at the output.

100
00:06:35.941 --> 00:06:40.361
And you discuss the number of models they
fit and the number of parameters they

101
00:06:40.361 --> 00:06:43.965
looked at, you can just kind of
do a kind of rough calculation,

102
00:06:43.965 --> 00:06:48.263
round it up to the easiest number to
multiply by, and use that correction.